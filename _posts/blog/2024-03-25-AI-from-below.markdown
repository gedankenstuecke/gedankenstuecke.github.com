---
title: "'AI' from below"
layout: post
date: 2024-03-25 15:40
tags: AI artificial-intelligence machine-learning citizen-science organising labor unions
blog: true
category: blog
---

[![James Scott & Georgia Aitkenhead sitting on panel on a stage, in the background a slide reading 'AUIK' and "Nothing about us without us"](/assets/images/2024-03-AIUK.jpg)](/assets/images/2024-03-AIUK.jpg)

Last week, I attended _["AI" UK](https://ai-uk.turing.ac.uk/)_ – which was organised by _The Alan Turing Institute_ and where I served in a minor role on the program advisory committee – and a workshop on [_Responsible "AI"_](https://www.responsible-ai.science/), co-organised by some fellow fellows of the [Software Sustainability Institute](https://www.software.ac.uk/). At both events I got the chance to talk about the different ways of implementing participatory approaches into how we do scientific research – and why doing so is important. And, somewhat continuing similar conversations at the _[The European Festival of Journalism and Media Literacy](https://tzovar.as/VOICES-festival/)_, there was increasing talk about doing and informing _data science from below_. 

At _"AI" UK_, one of the sessions in the very first slot was titled _Nothing About Us Without Us_. This panel included our own citizen science work on sensory processing and autism with AutSPACEs (represented by Georgia & James), as well as representatives of the [_People's Panel on AI_](http://connectedbydata.org/projects/2023-peoples-panel-on-ai), which _Connected by Data_ did in November last year. Their deliberative process included the public, represented by 12 participants, in envisioning "AI" policy making. Jointly, the panel outlined how participatory methods can be used to allow us to move from being data subjects into agents that can influence how decisions about data and its use are being made. 

This sentiment was mirrored, in slightly different form, at the session _Data, Labour and "AI"_. The panel was moderated by journalist Billy Perrigo, who discussed with sociologist Karen Gregory, Matt Buckley of [_United Tech & Allied Workers_](https://utaw.tech/about/) union branch, and Mophat Okinyi – who has worked in outsourced content moderation and is one of the founders of the [_African Content Moderators Union_](https://time.com/6275995/chatgpt-facebook-african-workers-union/). Mophat shared his first-hand experiences in doing content-moderation and how tech companies outsource this kind of work, in particular to the global south, while offering little in terms of pay and worker safety. The panel discussed how this type of _"hidden outsourcing"_ – be it for content moderation of social networks or training of language models – is done very deliberately, to hide the actual labor that goes into enabling any "AI" or automation. 

This type of hidden labor is in a way just [an extension of the longer tech history of fauxtomation](https://logicmag.io/failure/the-automation-charade/), in which "automation" is nothing but hidden workers doing the actual work at the end of the day. Karen Gregory noted that given the fact that there are plenty of workers with very intimate knowledge of how this "automation" works, it is bizarre that we keep talking to executives and sales people about the _risks of "AI"_, and not those with that first-hand experience from the other side: Relating this to her research with food delivery gig workers, she highlighted how those actually working within these technical systems are the ones that become the experts of these fields, effectively becoming _ethnographers of their work_. But their viewpoints remain marginalized, partially [because anthropology is kinda _out_ these days](https://zirk.us/@benjamingeer/112139416672527678), but also because colonialism and its friends aren't dead: Their work is done in the global south and then it also looks a lot like care work, which was historically and still remains highly undervalued. 

To me, all of these factors make the current _general purpose generative "AI"_ hype even more insidious: It's not only that the technology itself mostly remain a solution in desperate need of a problem, or that it is one that will be used to [foster further "deskilling"](http://perfors.net/blog/creation-ai/) (to paraphrase Sci-Fi author Adrian Tchaikovsky during his "AI" UK session: _Art is a craft that needs to be improved and outsourcing any part of your creative work means you are not improving your skills_). But it's that most, if not all, of these commercial tools are created using highly exploitative practices – from [getting the data from questionable sources with even more questionable consent](https://toot.cat/@zkat/112128845946356067) all the way to the labor practices that go into preparing those models. Which means that there's no way to ethically use any of these tools/models in the first place. And I don't think there will be a way any time soon – unless we (analogously to [Sandra Harding's _Sciences from Below_](https://www.dukeupress.edu/sciences-from-below/)) start doing "tech from below", beginning with listening to those who will be affected by these technologies and developing it collectively.